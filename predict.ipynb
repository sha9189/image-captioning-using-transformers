{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Image Caption Generator Using Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<a href=\"https://colab.research.google.com/github/sha9189/image-captioning-using-transformers/blob/master/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Click on **Open in Colab** button above to quickly run the model on any image of your choice.    \n",
    "<br>   \n",
    "This model's weights(500 MB) had to be uploaded on Google Drive due to the file-size limit of GitHub. Since Google Drive is a banned service within Allstate Network, it fails to load the model's weights when run on Allstate Network. Hence, it is adviced to try out this model on your **personal laptop**(and not on Allstate-issued machine).\n",
    "<br>   \n",
    "<br>   \n",
    "This is a preview of an Image Captioning System developed to describe different **scenes of a house** (like living room, bedroom, kitchen, bathroom, etc). Its architecture was influenced by the recent success of the [DETR](https://github.com/facebookresearch/detr) (DEtection-TRansformer) model on the task of Object Detection.   \n",
    "<br>   \n",
    "To run this Notebook, click inside the first cell below and repeatedly press `Shift+Enter` to progressively run the cells."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Download GitHub Repository and Setup Connection with Google Drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sha9189/image-captioning-using-transformers.git\n",
    "\n",
    "cd image-captioning-using-transformers/"
   ]
  },
  {
   "source": [
    "### Set up actual model end-to-end"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "try:\n",
    "    spacy_eng = spacy.load(\"en\")\n",
    "except:\n",
    "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "from configuration import Config\n",
    "from models import caption \n",
    "from datasets import coco\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model, _ = caption.build_model(config)\n",
    "\n",
    "    # load weights\n",
    "\n",
    "    model.backbone.load_state_dict(torch.load(\"checkpoints/checkpoint-breakdown/backbone.pth\", map_location='cpu'))\n",
    "    model.input_proj.load_state_dict(torch.load(\"checkpoints/checkpoint-breakdown/input_proj.pth\", map_location='cpu'))\n",
    "    model.transformer.load_state_dict(torch.load(\"checkpoints/checkpoint-breakdown/transformer.pth\", map_location='cpu'))\n",
    "    model.mlp.load_state_dict(torch.load(\"checkpoints/checkpoint-breakdown/mlp.pth\", map_location='cpu'))\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "\n",
    "    return caption_template, mask_template\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(image, caption, cap_mask):\n",
    "    model.eval()\n",
    "    for i in range(config.max_position_embeddings - 1):\n",
    "        predictions = model(image, caption, cap_mask)\n",
    "        predictions = predictions[:, i, :]\n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "        if predicted_id[0] == 3:\n",
    "            caption[:, i+1] = predicted_id[0]\n",
    "            return caption\n",
    "\n",
    "        caption[:, i+1] = predicted_id[0]\n",
    "        cap_mask[:, i+1] = False\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "\n",
    "def decode_caption(output, end_token):\n",
    "    sentence = []\n",
    "    for idx in output:\n",
    "        if idx == end_token:\n",
    "            break\n",
    "        word = english.vocab.itos[idx]\n",
    "        sentence.append(word)\n",
    "    # Remove <sos> from sentence\n",
    "    sentence = \" \".join(sentence[1:])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def predict_nb(image, model):\n",
    "    image = coco.val_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    start_token = english.vocab.stoi[\"<sos>\"]\n",
    "    end_token = english.vocab.stoi[\"<eos>\"]\n",
    "\n",
    "    caption, cap_mask = create_caption_and_mask(start_token, config.max_position_embeddings)\n",
    "    \n",
    "    image = image.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    caption = caption.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cap_mask = cap_mask.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    output = evaluate(image, caption, cap_mask)\n",
    "    output = output.tolist()[0]\n",
    "    output = decode_caption(output, end_token)\n",
    "    output = output.capitalize()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = load_model()\n",
    "english = torch.load('english.pth')"
   ]
  },
  {
   "source": [
    "To select image of your choice, follow these steps:   \n",
    "- Open [Google Images](https://images.google.com/) and search for any one of these - bedroom, kitchen, living room, bathroom.   \n",
    "- Click on image of your choice and select `Copy image address`.   \n",
    "- Run the below cell and paste it the link in the input box when prompted.  \n",
    "\n",
    "*To test multiple images, run only the below cell as many times as you'd like with different image addresses.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./images/copy_image_address.jpg\" alt=\"drawing\" width=\"500\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Please enter the image address below:\")\n",
    "url = input()\n",
    "image = Image.open(requests.get(url, stream=True).raw).resize((640,480)).convert('RGB')\n",
    "print(predict_nb(image, model))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}